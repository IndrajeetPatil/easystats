---
title: "Indices of Effect Existence and Significance in the Bayesian Framework"
output:
  word_document:
    toc: false
    toc_depth: 3
    df_print: "kable"
    highlight: "pygments"
    reference_docx: utils/Template_Frontiers.docx
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: utils/apa.csl
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = "")
knitr::opts_chunk$set(
  comment = ">",
  dpi = 300
  # ,fig.path = "figures/"
)
options(digits = 2)
```


# Abstract {.unnumbered}

There is now a general agreement that the Bayesian statistical framework is the right way to go for psychological science. Nevertheless, its flexible nature is its power and weakness, for there is no agreement about what indices should be computed or reported. Moreover, the lack of a consensual index of effect existence, such as the frequentist *p*-value, possibly contributes to the unnecessary murkiness that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several indices of effect existence, provide intuitive visual representation of the "behaviour" of such indices in relationship with common sources of variance such as sample size and frequentist significance. The results contribute to develop the intuitive understanding of the values that researchers report and allow to draw recommendations for Bayesian statistics description, critical for the standardization of scientific reporting. 

# Introduction

<!-- Introducing Bayes and issues related to p-value -->

The Bayesian framework is quickly gaining popularity among psychologists and neuroscientists [@andrews2013prior]. Reasons to prefer this approach are reliability, better accuracy in noisy data, better estimation for small samples, less proneness to type I error, the possibility of introducing prior knowledge into the analysis and, critically, results intuitiveness and their straightforward interpretation [@dienes2018four;@etz2016bayesian;@kruschke2010believe;@kruschke2012time;@wagenmakers2018bayesian;@wagenmakers2016bayesian]. The frequentist approach has been associated with the focus on null hypothesis testing, and the misuse of *p*-values has been shown to critically contribute to the reproducibility crisis of psychological science [@chambers2014instead; @szucs2016empirical]. There is a general agreement that the generalization of the Bayesian approach is a way of overcoming this issue [@benjamin2018redefine; @etz2016bayesian; @maxwell2015psychology; @wagenmakers2017need; @marasini2016use; @halsey2019reign].

<!-- when these sig indices are needed -->

While the probabilistic reasoning promoted by the Bayesian framework is pervading most of data science aspects, it is particularly well established for statistical modelling. This facet, on which psychological science massively rely, could roughly be grouped into two soft-edged categories; predictive and structural modelling. Although a statistical model can (often) serve both purposes, predictive modelling is devoted to build and find the best model that accurately predicts a given outcome. It is centered around the concepts such as fitting metrics, predictive accuracy and model comparison. At the extremum of this dimension lies deep learning models, used for their strong predictive power, sometimes at the expense of Human readability [these models has been often referred to as "black-boxes", emphasizing the difficulty to appraise their internal functioning; @burrell2016machine; @castelvecchi2016can; @snoek2012practical]. On the other side, psychologists are often using simpler models (for instance, based on the general linear framework) to explore their data. Within this framework, the goal switches from building the best model to understanding the parameters inside the model. Altough in reality, the pipeline often starts with predictive modelling ("what is the best model of the world, *i.e.*, the observed variable") and then seemingly transition to structural modelling ("given this model of the world, how are the effects, *i.e.*, the model parameters, influencing the outcome"), which conclusions often rely on some index of effect "significance".

<!--Estimation vs. significance -->

It is important to note that the importance and focus on significance assessment might differ accross fields. For instance, in applied physics or engineering, the goal of a study is often to precisely *estimate* (quantify) a given effect and its size (the question being, for example, wether a given coefficient is closer to 7 or to 9, or whether a difference is moderate or big). The Bayesian framework has demonstrated its superiorty in addressing this type of questions, its probabilistic framework leading to accurate estimations and allowing to quantify the inherent uncertainty associated with them. On the contrary, in other fields, such as for instance experimental psychology, the implicit aim has often been focused on *significance*: "are these two conditions different?", "is there a correlation between these two variables?". In other words, being able to conclude if a given parameter affects substantially the outcome with little care to its precise quantification.

<!-- Utility of significance -->

Despite a general agreement on its defects and recurring waves of attacks occuring for decades [@gardner1986confidence; anderson2000null; finch2004reform; fidler2004editors; @cohen2016earth], the tenacity and resilience of the *p*-value as an index of significance is remarkable, being still widely used and thaught (altough some journals have taken a radical step by banning them, *e.g.*, @gill_2018; expecting to shift the perspective to effect size estimation). This endurance might be informative on how such indices, and the accompanying heuristics to interpret them (*e.g.*, *.05*, *.01* and *.001*), are useful and necessary for researchers to gain an intuitive (although possibly simplified) understanding of the interactions and structure of their data. Moreover, this utility is salient in contexts where decisions have to be taken (*e.g.*, in medical settings). In spite of the statistical perspective, in which the absence of any hard mathematical thresholds or discrete categories naturally leads to the desire for a nuanced and subtle view, decisionners might often seek a threshold value to rationalize their acts. This practical need for significance assessment might be one of the mechanism fostering the wide adoption of the frequentist *p*-value and its arbitrary interpretation clusters. Unfortunately, these heuristics have severely rigidified, becoming a goal and threshold to reach rather than a tool for understanding the data [@cohen2016earth; @kirk1996practical]. 

<!-- How to improve sig testing -->

While significance testing (and its inherent categorical interpretation heuristics) might have its place alongside effect estimation as a complementary perspective, it does not preclude the fact that drastic improvements are needed. For instance, one possible advance could focus on improving the mathematical understanding (*e.g.*, through a new simpler index) of the values (as opposed to the obscure mathematical definition of the  *p*-value that contributes to its common misintepretation). Another improvement could be found in providing an intuitive (*e.g.*, visual) understanding of the behaviour of the indices in relationship with main sources of variance, such as sample size, noise or effect presence. Such better overall understanding of the indices would hopefully act as a barrier against their mindless reporting by allowing the users to nuance the interpretations and conclusions that they draw.


<!--Bayesian alternatives -->

So what does the Bayesian framework offer as alternatives for the *p*-value? Bayesian testing indices could be roughly grouped into three overlapping categories: Bayes factors, posterior indices and ROPE-based indices. Bayes factors reflect the predictive performance of one against another model [*e.g.*, the null vs. the alternative hypothesis; @jeffreys1998theory; @ly2016harold]. It provides many advantages over the *p*-value, having a straightforward interpretation and allowing to quantify the evidence in favour of the null hypothesis [@dienes2014using; @jarosz2014odds]. Nonetheless, its use for parameters description in complex models is still a matter of debate, being highly dependent on priors specification [@etz2018bayesian; @kruschke2018bayesian]. On the contrary, "posterior indices" reflect objective characteristics of the posterior distribution, such as for instance the proportion of strictly positive values. While the simplicity of their computation and interpretation is an asset, it could also be a limiting the information that they provide. Importantly, Bayes factors and indices derived solely from the posterior distribution are both the "natural, direct, and unavoidable consequence of Bayes' rule" [@rouder2018bayesian, p. 106]. Finally, ROPE-based indices are related to the redefinition of the point-null hypothesis [@kruschke2014doing; @lakens2017equivalence; @lakens2018equivalence] as a range of values (the Region of Practical Equivalence - ROPE), usually equally spread around 0 (*e.g.*, [-0.1; 0.1]) and considered as negligible (*i.e.*, too small to be of practical relevance). It is interesting to note that this perspective unites Bayesian indices with the focus on effect size (involving a discrete separation between at least two categories), which finds an echo in the recent statistical recommendations [@sullivan2012using; @ellis2003practical; @simonsohn2014p].


<!-- Research gap statement -->

Curiously, despite the richness provided by the Bayesian framework and the availability of multiple indices, no consensus has yet emerged on the ones to use. On the contrary, the litterature continues to bloom in a raging debate, often polarized between proponents of the Bayes factor as the supreme index and its detractors [*e.g.*, @spanos2013should; @robert2014jeffreys; robert2016expected; @wagenmakers2015another], with strong theoretical arguments being developped on both sides. Unfortunately, no practical, empirical and direct comparison between these indices has ever been done. This might be a rebuttal for scientists interested in adopting the Bayesian framework. Moreover, this grey area can increase the difficulty of readers or reviewers unfamiliar with the Bayesian framework to follow the assumptions and conclusions, which could in turn generate unnecessary doubt upon the entire study. While we think that such indices of significance and their interpretation guidelines (in the form of rules of thumb) are useful in practice, we also strongly believe that they should be accompanied with the understanding of their "behaviour" in relationship with major sources of variance, such as sample size and noise. This knowledge is important for people to implicitly and intuitively appraise the meaning and implication of the mathematical values they report. This could, in turn, prevent the crystallization of the possible heuristics and categories derived from such indices.

<!-- Aim and hypotheses -->

Thus, based on the simulation of multiple linear and logistic regressions (arguably some of the most widely used models), the present work aims at comparing several indices of effect "significance", provide visual representations of the "behaviour" of such indices in relationship with sample size, noise and effect presence, as well as the frequentist *p*-value (an index which, beyond its many flaws, is well known and could be used as a reference for Bayesian neophytes), and finally draw recommendations for Bayesian statistics reporting.




# Methods

## Data Simulation

The simulation of datasets suited for linear or logistic regression, we started by simulating an independent, normally distributed *x* variable (with mean 0 and SD 1) of a given sample size. Then, the corresponding *y* variable was added, having a perfect correlation (in the case of data for linear regressions) or as a binary variable perfect separated by *x* (in the case of no effect, the *y* variable was created independent of *x*). Finally, a Gaussion noise was added (the error).

The simulation aimed at modulating the following characteristics: *data type* (*i.e.*, used for linear or logistic regression), *Sample size* (from 20 to 100 by steps of 10), *"true" effect* (original regression coefficient from which data is drawn prior to noise addition, 1 - presence of effect or 0 - absence of effect) and *noise* (Gaussian noise applied to the predictor with SD uniformly spread between 0.666 and 6.66, with 1000 different values). We generated a dataset for each combination of these characteristics, resulting in a total of 36000 (`2 * 2 * 9 * 1000`) datasets. The code used for generation is available on Github (https://github.com/easystats/easystats/tree/master/publications/makowski_2019_bayesian/data). Please note that it takes usually several days/weeks for the generation to complete.

```{r DataLoad, message=FALSE, warning=FALSE, echo=FALSE}
library(dplyr)
library(tidyr)

# df <- read.csv("https://raw.github.com/easystats/easystats/master/publications/makowski_2019_bayesian/data/data.csv")
df <- read.csv("../data/data.csv")
df <- df[seq(1,nrow(df), length.out = 3600),] # get every 10th line
```


## Indices

For each of these datasets, Bayesian and frequentist regressions were fitted to predict *y* with *x* as a unique predictor. The Bayesian models had default, mildly informative priors (normal distribution with mean 0 and SD 1) over the parameter of interest, and were fitted using MCMC (4 chains of 2000 iterations, half of which used for warm-up). For all of the simulated models, we computed seven indices related to the effect of *x*. These are availabe and described in details in the *bayestestR* R package **(BAYESTESTR CITATION)**.

### Frequentist *p*-value

Based on the frequentist regression, this index represents the probability for a given statistical model that, when the null hypothesis is true, the effect would be greater than or equal to the actual observed results [@wasserstein2016asa].

### Probability of Direction (*pd*)

The Probability of Direction (*pd*) varies between 50\% and 100\% and can be interpreted as the probability that a parameter (described by its posterior distribution) is strictly positive or negative (whichever is the most probable). It is mathematically defined as the proportion of the posterior distribution that is of the median's sign **(BAYESTESTR CITATION)**.

### MAP-based *p*-value

The *MAP-based p-value* (also refered to as the "Bayesian *p*-value") is related to the odds that a parameter has against the null hypothesis [@mills2014bayesian; @mills2017objective]. It is mathematically defined as the density value at 0 divided by the density at the Maximum A Posteriori (MAP), *i.e.*, the equivalent of the mode for continuous distributions.

### ROPE (95\%) and ROPE (full)

The *ROPE (95\%)* refers to the percentage of the 95\% HDI that lies within the ROPE. As suggested by @kruschke2014doing, the Region of Practical Equivalence (ROPE) was defined as ranging from -0.1 to 0.1 for linear regressions and its equivalent, -0.18 to -0.18, for logistic models [based on the $π/√(3)$ formula to convert log odds ratios to standardized differences; @cohen1988statistical]. The *ROPE (full)* refers to the percentage of the whole posterior distribution that lies within the ROPE.

### Bayes factor (*vs.* 0) and Bayes factor (*vs.* ROPE)

The Bayes Factor (*BF*) used here is based on prior and posterior distributions of a single parameter. In this context, the Baye factor indicates the degree by which the mass of the posterior distribution has shifted further away from or closer to the null value(s) (relative to the prior distribution), thus indicating if the null hypothesis has become less or more likely given the observed data. We created two indices corresponding to two definitions for the null. In the case of testing against a point null (0), a Savage-Dickey density ratio was computed, which is also an approximation of a Bayes factor comparing the marginal likelihoods of the model against a model in which the tested parameter has been restricted to the point null [@wagenmakers2010bayesian]. We also computed the BF of the posterior distribution against the range of negligible values (same as for the ROPE), by comparing the prior and posterior odds of the parameter falling within vs. outside the ROPE [see *Non-overlapping Hypotheses* in @morey2011bayesinterval].

## Data Anaysis

The aim of this study is two-fold: 1) compare indices of effect existence and significance between them, 2) provide visual guides to provide an intuitive understanding of the numeric values in relation with a known frame of reference (the frequentist NHST framework). Thus, we will start by 1) presenting the relationship between these indices and main sources of variance, such as effect existence, sample size and noise. 2) Compare Bayesian indices with the frequentist *p*-value and its commonly used thresholds (.05, .01, .001). Taken together, these results will help us to outline numeric guides to ease the reporting and interpretation of the indices.

# Results

## Impact of Sample Size and Noise

```{r Figure1, message=FALSE, warning=FALSE, fig.cap="Figure 1. Impact of Sample Size.", fig.align='center', fig.width=21/2, fig.height=29.7/2, echo=FALSE}

knitr::include_graphics("figures/Figure1.png")
```

<!-- MSB: Move disccusion of consistancy here? -->

```{r Figure2, message=FALSE, warning=FALSE, fig.cap="Figure 2. Impact of Noise.", fig.align='center', fig.width=21/2, fig.height=29.7/2, echo=FALSE}

knitr::include_graphics("figures/Figure2.png")
```




```{r Table1, message=FALSE, warning=FALSE, echo=FALSE}
library(parameters)
library(performance)

table1 <- data.frame()
for (model_type in c("linear", "binary")) {
  p_value <- df %>%
    parameters::normalize(select = "p_value") %>%
    mutate(p_value = 1 - p_value) %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ p_value + error + sample_size, data = ., family = "binomial")
  p_direction <- df %>%
    parameters::normalize(select = "p_direction") %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ p_direction + error + sample_size, data = ., family = "binomial")
  p_MAP <- df %>%
    parameters::normalize(select = "p_MAP") %>%
    mutate(p_MAP = 1 - p_MAP) %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ p_MAP + error + sample_size, data = ., family = "binomial")
  ROPE_95 <- df %>%
    parameters::normalize(select = "ROPE_95") %>%
    mutate(ROPE_95 = 1 - ROPE_95) %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ ROPE_95 + error + sample_size, data = ., family = "binomial")
  ROPE_full <- df %>%
    parameters::normalize(select = "ROPE_full") %>%
    mutate(ROPE_full = 1 - ROPE_full) %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ ROPE_full + error + sample_size, data = ., family = "binomial")
  BF_log <- df %>%
    parameters::normalize(select = "BF_log") %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ BF_log + error + sample_size, data = ., family = "binomial")
  BF_ROPE_log <- df %>%
    parameters::normalize(select = "BF_ROPE_log") %>%
    filter(outcome_type == model_type) %>%
    glm(true_effect ~ BF_ROPE_log + error + sample_size, data = ., family = "binomial")

  performance_table <- performance::compare_performance(p_value, p_direction, p_MAP, ROPE_95, ROPE_full, BF_log, BF_ROPE_log)
  performance_table <- cbind(data.frame(Model_Type = model_type), performance_table)
  table1 <- rbind(table1, performance_table)
}

table1 %>%
  select(Model_Type, Model, AIC, BIC, R2_Tjur, RMSE, LOGLOSS, PCP, BF) %>%
  mutate(BF_log = bayestestR:::.format_big_small(log(BF)),
         BF = bayestestR:::.format_big_small(BF)) %>%
  knitr::kable(digits = 2, caption = "Table 1. Performance comparison of the indices in predicting the presence of an effect.")
```


For each index and each model type, we fitted a (frequentist) logistic regression to predict the presence or absence of effect, adjusted for noise and sample size. The comparison of the performance of these models (AIC, BIC and Tjur's R2) revealed a consistent pattern accross model type (*i.e.*, similar for linear and logistic models), suggesting that *BF (vs. ROPE)* is the best index to discriminate between the presence and the absence of an effect, followed by *BF (vs. 0)*, *ROPE (full)*, *ROPE (95\_%)*, *p-MAP*, *p-direction* and the frequentist *p*-value. The Bayes factor [against the frequentist *p*-value model, computed via BIC approximation @wagenmakers2007practical], used here as a measure of relative performance, supported this conclusion.

<!-- MSB: Looking at the models, we can talk about how p-values have some discriminatory power, but how the other indices are much better (besides pd) -->

<!-- MSB: interval BFs descriminate better between null (or negligable) and true (un-negligable) effects [@morey2011bayesinterval; rouder2012default] -->


## Relationship with the frequentist *p*-value



```{r Figure3, message=FALSE, warning=FALSE, fig.cap="Figure 3. Relationship with the frequentist p-value.", fig.align='center', fig.width=21/2, fig.height=29.7/2, echo=FALSE}

knitr::include_graphics("figures/Figure3.png")

```

<!-- MSB: in each plot, the markers on the top (null effect) and bottom (true effect) represent the density of p-values -->
<!-- MSB: whereas on the right (null effect) and left (true effect) they represent the density of the index of interest -->

**Figure 3** suggests that the *pd* has a 1:1 correspondance with the frequentist *p*-value. *BF* indices still appear as having a strong relationship (altough severly non-linear) with the frequentist index, which is to be expected since the *BF* is consistant (i.e., it approaches the "true" bound as sample size increases) both when the null is true and when the alternative is ture (as can be seen in the marginal distibutions, marked by colored dashes in the plot margins), whereas the *p*-value is only consistant when the alternative is true, but has a uniform distribution [0-1] when the null is true [@rouder2012default; @rouder2009bayesian]. *ROPE*-based percentages appear to be only weakly related to *p*-values.



TODO: Add points type as sample size as ROPE seems to be directly related to groups of sample size.


```{r Figure4, message=FALSE, warning=FALSE, fig.cap="Figure 4. Relationship between Bayesian indices and frequentist arbitrary threshold of significance.", fig.align='center', fig.width=29.7/2.5, fig.height=21/2.5, echo=FALSE}
# df$sig_1 <- factor(ifelse(df$p_value >= .1, "n.s.", "*"), levels = c("n.s.", "*"))
# df$sig_05 <- factor(ifelse(df$p_value >= .05, "n.s.", "*"), levels = c("n.s.", "*"))
# df$sig_01 <- factor(ifelse(df$p_value >= .01, "n.s.", "*"), levels = c("n.s.", "*"))
# df$sig_001 <- factor(ifelse(df$p_value >= .001, "n.s.", "*"), levels = c("n.s.", "*"))
# 
# 
# make_figure4 <- function(.data) {
#   .data$level <- ifelse(.data$level == "n.s.", 0, 1)
#   fit <- suppressWarnings(glm(level ~ value, data = .data, family = "binomial"))
#   newdata <- data.frame(value = seq(min(.data$value), max(.data$value), length.out = 500))
#   newdata$sig <- predict(fit, newdata = newdata, type = "response")
#   newdata
# }
# 
# figure4_data <- df %>%
#   gather("threshold", "level", sig_1:sig_001) %>%
#   gather("index", "value", p_direction, p_MAP, ROPE_95, ROPE_full, BF_log, BF_ROPE_log) %>%
#   group_by(outcome_type, index, threshold) %>%
#   nest() %>%
#   mutate(data = lapply(data, make_figure4)) %>%
#   unnest() %>%
#   ungroup() %>%
#   mutate(
#     threshold = factor(threshold, levels = c("sig_1", "sig_05", "sig_01", "sig_001")),
#     index = factor(index, levels = c("p_direction", "p_MAP", "ROPE_95", "ROPE_full", "BF_log", "BF_ROPE_log"))
#   ) %>%
#   filter(
#     (index == "p_direction" & value >= 0.925) | index != "p_direction",
#     (index == "p_MAP" & value <= 0.35) | index != "p_MAP",
#     (index == "ROPE_95" & value <= 0.3) | index != "ROPE_95",
#     (index == "ROPE_full" & value <= 0.3) | index != "ROPE_full",
#     (index == "BF_log" & value <= 8) | index != "BF_log",
#     (index == "BF_ROPE_log" & value <= 8) | index != "BF_ROPE_log"
#   )
# 
# 
# figure4 <- ggplot(figure4_data, aes(x = value, y = sig, linetype = threshold, group = interaction(outcome_type, threshold))) +
#   geom_line(aes(color = outcome_type), size = 1) +
#   facet_wrap(~index, scales = "free", labeller = as_labeller(indices)) +
#   theme_modern() +
#   scale_color_manual(values = c(`linear` = "#2196F3", `binary` = "#FF9800"), name = "Model Type", labels = c(`linear` = "Linear", `binary` = "Logistic")) +
#   scale_linetype_manual(
#     values = c("sig_1" = "dotted", "sig_05" = "dashed", "sig_01" = "longdash", "sig_001" = "solid"),
#     labels = c("p < .1", "p < .05", "p < .01", "p < .001"),
#     name = "Threshold"
#   ) +
#   ylab("Probability of being significant") +
#   xlab("Index Value") +
#   coord_cartesian(ylim = c(0, 1))
# figure4
# # ggsave("figure4.png", figure4, height = 8, width = 6)
```




```{r table2, message=FALSE, warning=FALSE, echo=FALSE}
table2 <- rbind(
  data.frame(
    "Index" = "Probability of Direction (pd)",
    "Interpretation" = "Probability that an effect is of the same sign as the median's.",
    "Definition" = "Proportion of the posterior distribution of the same sign than the median's.",
    "Strengths" = "Straightforward computation and interpretation. Objective property of the posterior distribution. 1:1 correspondance with the frequentist p-value.",
    "Limitations" = "Limited information favouring the null hypothesis."
    ),
  data.frame(
    "Index" = "MAP-based p-value",
    "Interpretation" = "Relative odds of the presence of an effect against 0.",
    "Definition" = "Density value at 0 divided by the density value at the mode of the posterior distribution.",
    "Strengths" = "Straightforward computation. Objective property of the posterior distribution",
    "Limitations" = "Limited information favouring the null hypothesis. Relates on density approximation. Indirect relationship between mathematical definition and interpretation."
    ),
  data.frame(
    "Index" = "ROPE (95%)",
    "Interpretation" = "Probability that the credible effect values are not negligible.",
    "Definition" = "Proportion of the 95% CI inside of a range of values defined as the ROPE.",
    "Strengths" = "Provides information related to the practical relevance of the effects.",
    "Limitations" = "A ROPE range needs to be arbitrarily defined. Sensitive to the scale (the unit) of the predictors. Not sensitive to highly significant effects."
    ),
  data.frame(
    "Index" = "ROPE (full)",
    "Interpretation" = "Probability that the effect possible values are not negligible.",
    "Definition" = "Proportion of the posterior distribution inside of a range of values defined as the ROPE.",
    "Strengths" = "Provides information related to the practical relevance of the effects.",
    "Limitations" = "A ROPE range needs to be arbitrarily defined. Sensitive to the scale (the unit) of the predictors."
    ),
  data.frame(
    "Index" = "Bayes factor (vs. 0)",
    "Interpretation" = "???",
    "Definition" = "???",
    "Strengths" = "???",
    "Limitations" = "???"
    ),
  data.frame(
    "Index" = "Bayes factor (vs. ROPE)",
    "Interpretation" = "???",
    "Definition" = "???",
    "Strengths" = "???",
    "Limitations" = "A ROPE range needs to be arbitrarily defined. Sensitive to the scale (the unit) of the predictors."
    )
)

knitr::kable(table2, digits = 2, caption = "Table 2. Summary of Bayesian Indices of Effect Existence and Significance.")
```


## Relationship between ROPE (full), pd and BF (vs. ROPE)

```{r Figure5, message=FALSE, warning=FALSE, fig.cap="Figure 5. Relationship between three Bayesian indices.", fig.align='center', fig.width=21/1.8, fig.height=29.7/1.8, echo=FALSE}
# pd_rope <- df %>%
#   mutate(true_effect = as.factor(true_effect)) %>%
#   ggplot(aes(x = p_direction, y = ROPE_full, color = sample_size, shape = outcome_type, alpha = true_effect)) +
#   geom_point() +
#   scale_shape_manual(values = c(1, 3), name = "Model Type", labels = c(`linear` = "Linear", `binary` = "Logistic")) +
#   scale_alpha_manual(values = c(0.3, 1), name = "Effect", labels = c(`0` = "Absence", `1` = "Presence")) +
#   scale_color_material_c(palette = "rainbow", name = "Sample Size") +
#   ylab("ROPE (full)") +
#   xlab("Probability of Direction (pd)") +
#   theme_modern()
# 
# pd_bf <- df %>%
#   filter(BF_ROPE_log <= log(999)) %>%
#   mutate(true_effect = as.factor(true_effect)) %>%
#   ggplot(aes(x = p_direction, y = BF_ROPE_log, color = sample_size, shape = outcome_type, alpha = true_effect)) +
#   geom_point() +
#   scale_shape_manual(values = c(1, 3), name = "Model Type", labels = c(`linear` = "Linear", `binary` = "Logistic")) +
#   scale_alpha_manual(values = c(0.3, 1), name = "Effect", labels = c(`0` = "Absence", `1` = "Presence")) +
#   scale_color_material_c(palette = "rainbow", name = "Sample Size") +
#   ylab("log BF (vs. ROPE)") +
#   xlab("Probability of Direction (pd)") +
#   theme_modern()
# 
# rope_bf <- df %>%
#   filter(BF_ROPE_log <= log(999)) %>%
#   mutate(true_effect = as.factor(true_effect)) %>%
#   ggplot(aes(x = ROPE_full, y = BF_ROPE_log, color = sample_size, shape = outcome_type, alpha = true_effect)) +
#   geom_point() +
#   scale_shape_manual(values = c(1, 3), name = "Model Type", labels = c(`linear` = "Linear", `binary` = "Logistic")) +
#   scale_alpha_manual(values = c(0.3, 1), name = "Effect", labels = c(`0` = "Absence", `1` = "Presence")) +
#   scale_color_material_c(palette = "rainbow", name = "Sample Size") +
#   ylab("log BF (vs. ROPE)") +
#   xlab("ROPE (full)") +
#   theme_modern()
# 
# figure5 <- see::plots(pd_rope, pd_bf, rope_bf, nrow = 3)
# figure5 # How to remove this annoying TableGrob text?!
# ggsave("figure5.png", figure5, height = 8, width = 6)
```


The **Figure 5** suggests that the relationship between the *ROPE (full)* and the *pd* might be strongly affected by the sample size, and the relationship between *BF (vs. ROPE)* and the *pd* might be subject to differences accross model types. Moreover, the *ROPE (full)* and the *BF (vs. ROPE)* seem very closely related within the same model type. These results reflect *ROPE (full)* and *BF (vs. ROPE)*'s consitancy both when the null is true and when the alternative is true, where the *pd*, being equivalent to the *p*-value, is only consistant when the null is true.


# Discussion

<!-- /// Summary of aim and methods -->

Based on the simulation of multiple linear and logistic models, the present work aimed at comparing several Bayesian indices of effect existence, provide visual representations of the "behaviour" of such indices in relationship with important sources of variance such as sample size, noise, presence of effect, as well as comparing them with the well-known and widely used frequentist *p*-value and its arbritrary interpretation heuristics.


<!-- /// Results dicsussion -->
<!-- Sensibility to sample size -->

The results tend to suggest that 1) indices could be separated into two categories.

<!-- Sensibility to noise -->

Noise.

<!-- Sensibility to effect existence -->

Discrimination analysis.


<!-- Comparison to p-value -->

What is the point of comparing Bayesian indices with the frequentist *p*-value, especially after having pointed out to its many flaws? Indeed, while this comparison may seem counterintuitive or wrong (as the Bayesian thinking is intrinsically different from the frequentist framework), we believe that this juxtaposition is interesting for didactic reasons. The frequentist *p*-value "speaks" to many and can thus be seen as a reference and a way to facilitate the shift toward the Bayesian framework. Thus, pragmatically documenting such bridges can only foster the understanding of the methodological issues that our field is facing, and in turn act against the sectarism and isolation caused by a dogmatic approach to a framework. This does not preclude, however, that a change in the general paradigm of significance seeking in necessary, and that Bayesian indices are fundamentally different from the frequentist *p*, rather than mere approximations or equivalents.


<!-- /// General discussion -->
<!-- Significance vs. Existence -->

Critically, while the purpose of these indices was solely termed as *significance* until now, we would like to emphasize the nuanced perspective of the existence-significance testing as a dual-framework for parameters description and interpretation. The idea supported here is that there is a conceptual and practical distinction, and possible dissociation to be made, between an effect's *existence* and *significance*. In this context, existence is simply defined as the consistency of an effect in one particular direction (*i.e.*, positive or negative), without any assumptions or conclusions as to its size, importance or meaning. It is an objective feature of an estimate (tied to its uncertainty). On the other hand, *significance* would be here reframed following its original literally definition ("being worthy of attention; importance"), which a neutral approach would link with the concept of effect size. An effect can be considered significant if its magnitude is higher than a given threshold. This aspect can be explored, to a certain extent, in an objective way with the concept of *practical equivalence* [@kruschke2014doing; @lakens2017equivalence; @lakens2018equivalence, which suggests the use of a range of values assimilated to absence of effect (the ROPE). If the effect falls within this range, it is considered as non-significant *for practical reasons*: the magnitude of the effect is likely to be too small to be of paramount importance in real-world scenarios. Nevertheless, *significance* also withholds are more subjective aspect, corresponding to its contextual meaningfulness, which is highly dependent on the literature, priors, novelty, context or field, and thus cannot be neutrally assessed with any statistical index. Interstingly, the weight of one or the other aspect of this framework might depend on the question asked. For instance, in a study exploring the effects of a new treatment, the focus might be *existence*: how much are we certain that the effect is beneficial and not detreminental? In a furhter step, however, the researcher might become itnerested in *significance*: is this effect large enough to be of any interest? Acknowledging the distinction and complementarity of these two aspects could in turn enrich the information and usefulness of the results reported in psychological science. For practical reasons, the implementation of EXIT (Effect eXistence and sIgnificance Testing) is made straightforward through the *bayestestR* open-source package for R (**BAYESTESTR CITATION**).

<!-- DM: NHST and CET are dead, long live the EXIT (Existence sIgnificance Testing) [literally just made that up, a better acronym might possibly be found ^^] -->

Critically to the aim of that paper, the EXIT dual-perspective spontaneously stems out from the probabilistic nature of the Bayesian framework, which allows these two aspects of parameters assessment to coexist and yet be neatly delineated. Moreover, the distinction between *existence* and *significance* is also supported by the empirical data presented in this paper, in regards to the sensitivity to the indices to the amount of evidence (sample size). In summary, 

As these twe two aspects of EXIT are complementary, we suggest using at minimum one index of each category.


Thus, an effect will be comprehensively reported if, beyond its estimation [with a point estimate, such as the median, and an index of uncertainty, such as the 89\% Credible Interval; @mcelreath2018statistical], it presents an index of existence

researchers should rely on statistics to assess effect existence (as well as size and direction estimation), and systematically, but contextually, discuss its meaning and importance within a larger perspective.

<!-- Comparison between indices -->
<!-- The pd -->

After all the criticism regarding the frequentist *p*-value, it might appear as counterintuitive to suggest the usage of its Bayesian empirical equivalent. The more subtle perspective that we support is that the *p*-value is not an intrinsically bad, or wrong, index. Instead, it is its misuse, misunderstanding and misinterpretation that fuels, in our opinion, the decay of the situation into the crisis. Interestingly, the proximity between the *pd* and the the *p*-value suggests that the latter is more an index of effect *existence* than *significance* (*i.e.*, "worth of interest"). Addressing this confusion, the Bayesian equivalent has an intuitive meaning and interpretation, making also obvious the fact that all thresholds and heuristics are arbitrary. Additionally, its mathematical and interpretative transparency of the *pd*, and its reconceptualisation as an index of effect existence, offers a valuable insight into the characterization of Bayesian results, and its practical proximity with the frequentist *p*-value makes it a perfect metric to ease the transition of psychological research into the adoption of the Bayesian framework.




# Conclusion and Limitations

How these observations can be used to improve statistical good practices in psychological science? Before being able to draw a definitive conclusion about the qualities of these indices, further studies need to investigate the robustness of these indices to sampling characteristics (*e.g.*, samplign algorithm, number of iterations, chains, warm-up) and the impact of prior specification [@kass1995bayes; @kruschke2011bayesian], which might be concidered a feature of the Bayesian framework [@vanpaemel2010prior]. <!-- MSB: All of these refs are about the BF, as ROPE etc are not affected by the how "wide" the priors are, but instead by how "narrow" they are - what I called the reversed Jeffreys-Lindley-Bartlett paradox: https://github.com/easystats/blog/issues/25#issuecomment-503391174 -->

One of the limitation the ROPE-based indices is that they require an explicit definition of the ROPE range. Importantly, as this range is fixed to the scale of the outcome response, these indices are thus sensitive to changes in the scale of the predictors.

Thus, in order to assess the existence and significance of effects within a regession model, we recommend to report, at minimum, the *pd* as an index of effect existence and the *BF (vs. ROPE)* as an index of significance. The former for its simplicity of interpretation, its robustness and its numeric proximity to the well-known frequentist *p*-value, and the latter for its ability to discriminate between presence and absence of effect [@de2007alternative], and the information it provides related to relative evidence of the size of the effect. <!-- MSB: BF-ROPE is most sensitive to effect **existance** as found in the sensitivity analysis! And IMO, it is also simple to interp -->
<!-- MSB: Should add a sample sentance for how to report -->


<!-- - ROPE based indices seems to be the most sensitive to different model types, as equivalent ROPE ranges are not straightforward to select. -->
<!-- - ROPE seems also strongly affected by sample {MSB: this is a feature - consistancy!} -->
<!-- - pd and BF might be two relevant indices, being both intuitive, providing different information. {MSB: when it is hard to specify the ROPE, or good prior - due to weak theory, or prior knowlage in the field of reaserch, I think the pd is the minimal measure to report, as it requires / is affected by none of these. But... in essense by recommending the pd, we're recommending a "basic" p-value, no? Should address that Bayes modeling gives the ability to precisly explore possible values of the parameter space...}-->
<!-- - On the other hand, BF and pd seems quite closely related {MSB: How so?}: ROPE might give a different perspective, altough to be taken with caution. {MSB: agree}-->

# Supplementary Materials

The full R code used for data generation, data processing, figures creation and manuscript compiling is available on Github at https://github.com/easystats/easystats/tree/master/publications/makowski_2019_bayesian.

# Acknowledgments

This study was made possible by the developpment of the **bayestestR** package, itself part of the [*easystats*](https://github.com/easystats/easystats) ecosystem, an open-source and collaborative project created to facilitate the usage of R. Thus, we would like to thank the [council of masters](https://github.com/orgs/easystats/people) of easystats, all other padawan contributors, as well as the users.


# References
